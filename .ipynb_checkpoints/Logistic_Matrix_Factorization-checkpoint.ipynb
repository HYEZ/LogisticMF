{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_matrix(filename, num_users, num_items):\n",
    "    t0 = time.time()\n",
    "    counts = np.zeros((num_users, num_items))\n",
    "    total = 0.0\n",
    "    num_zeros = num_users * num_items\n",
    "    for i, line in enumerate(open(filename, 'r')):\n",
    "        user, item, count = line.strip().split('\\t')\n",
    "        user = int(user)\n",
    "        item = int(item)\n",
    "        count = float(count)\n",
    "        counts[user][item] = count\n",
    "        total += count\n",
    "        num_zeros -= 1\n",
    "    alpha = num_zeros / total\n",
    "    print ('alpha %.2f' % alpha)\n",
    "    counts *= alpha\n",
    "    t1 = time.time()\n",
    "    print ('Finished loading matrix in %f seconds' % (t1 - t0))\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticMF():\n",
    "\n",
    "    def __init__(self, counts, num_factors, reg_param=0.6, gamma=1.0,\n",
    "                 iterations=30):\n",
    "        self.counts = counts\n",
    "        self.num_users = counts.shape[0] # 유저 수\n",
    "        self.num_items = counts.shape[1] # 아이템 수\n",
    "        self.num_factors = num_factors # latnet factor 수\n",
    "        self.iterations = iterations # interation 수\n",
    "        self.reg_param = reg_param # 정규화항\n",
    "        self.gamma = gamma # learning rate\n",
    "\n",
    "    # 학습\n",
    "    # log 확률을 최대화시키는 x,y, 베타를 찾아야하므로 gradient ascent 진행\n",
    "    def train_model(self):\n",
    "\n",
    "        self.ones = np.ones((self.num_users, self.num_items)) # (user 수 * item 수) 의 값이 1인 행렬 만들기, 논문에서 R\n",
    "        self.user_vectors = np.random.normal(size=(self.num_users,\n",
    "                                                   self.num_factors)) # (유저 수 * latent factor) 의 유저 행렬 만들기, 논문에서 X\n",
    "        self.item_vectors = np.random.normal(size=(self.num_items,\n",
    "                                                   self.num_factors)) # (아이템 수 * latent factor)의 아이템 행렬 만들기, 논문에서 Y\n",
    "        self.user_biases = np.random.normal(size=(self.num_users, 1)) # 유저 수만큼의 유저 편향 백터, 논문에서 베타u\n",
    "        self.item_biases = np.random.normal(size=(self.num_items, 1)) # 아이템 수만큼의 아이템 편향 벡터, 논문에서 베타i\n",
    "\n",
    "        user_vec_deriv_sum = np.zeros((self.num_users, self.num_factors)) # 유저 행렬의 미분 0으로 초기화\n",
    "        item_vec_deriv_sum = np.zeros((self.num_items, self.num_factors)) # 아이템행렬의 미분 0으로 초기화\n",
    "        user_bias_deriv_sum = np.zeros((self.num_users, 1)) # 유저 편향 벡터의 미분 0으로 초기화\n",
    "        item_bias_deriv_sum = np.zeros((self.num_items, 1)) # 아이템 편향 벡터의 미분 0으로 초기화\n",
    "        \n",
    "        # iterations만큼 학습\n",
    "        # 미분해서 추정값을 구할 때 Alternating Least Squares (ALS) 알고리즘을 사용한다.\n",
    "        # -> 파라미터 추정 시 유저 파라미터(x, 베타u)와 아이템 파라미터(y, 베타i)를 번갈아가면서 추정한다.\n",
    "        for i in range(self.iterations):\n",
    "            t0 = time.time()\n",
    "            # Fix items and solve for users\n",
    "            # take step towards gradient of deriv of log likelihood\n",
    "            # we take a step in positive direction because we are maximizing LL\n",
    "            # 아이템 파라미터를 고정시키고 유저 파라미터를 추정한다.\n",
    "            user_vec_deriv, user_bias_deriv = self.deriv(True) # 유저 파라미터의 미분 값을 구한다.\n",
    "            # AdaGrad 수행\n",
    "            user_vec_deriv_sum += np.square(user_vec_deriv) \n",
    "            user_bias_deriv_sum += np.square(user_bias_deriv)\n",
    "            vec_step_size = self.gamma / np.sqrt(user_vec_deriv_sum)\n",
    "            bias_step_size = self.gamma / np.sqrt(user_bias_deriv_sum)\n",
    "            self.user_vectors += vec_step_size * user_vec_deriv # 유저 파라미터 값 갱신\n",
    "            self.user_biases += bias_step_size * user_bias_deriv\n",
    "\n",
    "            # Fix users and solve for items\n",
    "            # take step towards gradient of deriv of log likelihood\n",
    "            # we take a step in positive direction because we are maximizing LL\n",
    "            # 유저 파라미터를 고정시키고 아이템 파라미터를 추정한다.\n",
    "            item_vec_deriv, item_bias_deriv = self.deriv(False) # 아이템 파라미터의 미분 값을 구한다.\n",
    "            # AdaGrad 수행\n",
    "            item_vec_deriv_sum += np.square(item_vec_deriv)\n",
    "            item_bias_deriv_sum += np.square(item_bias_deriv)\n",
    "            vec_step_size = self.gamma / np.sqrt(item_vec_deriv_sum)\n",
    "            bias_step_size = self.gamma / np.sqrt(item_bias_deriv_sum)\n",
    "            self.item_vectors += vec_step_size * item_vec_deriv # 아이템 파라미터 값 갱신\n",
    "            self.item_biases += bias_step_size * item_bias_deriv\n",
    "            \n",
    "            t1 = time.time()\n",
    "            print('iteration %i finished in %f seconds' % (i + 1, t1 - t0))\n",
    "\n",
    "    # 미분\n",
    "    def deriv(self, user):\n",
    "        if user:\n",
    "            vec_deriv = np.dot(self.counts, self.item_vectors)\n",
    "            bias_deriv = np.expand_dims(np.sum(self.counts, axis=1), 1)\n",
    "\n",
    "        else:\n",
    "            vec_deriv = np.dot(self.counts.T, self.user_vectors)\n",
    "            bias_deriv = np.expand_dims(np.sum(self.counts, axis=0), 1)\n",
    "        A = np.dot(self.user_vectors, self.item_vectors.T)\n",
    "        A += self.user_biases\n",
    "        A += self.item_biases.T\n",
    "        A = np.exp(A)\n",
    "        A /= (A + self.ones)\n",
    "        A = (self.counts + self.ones) * A\n",
    "\n",
    "        if user:\n",
    "            vec_deriv -= np.dot(A, self.item_vectors)\n",
    "            bias_deriv -= np.expand_dims(np.sum(A, axis=1), 1)\n",
    "            # L2 regularization\n",
    "            vec_deriv -= self.reg_param * self.user_vectors\n",
    "        else:\n",
    "            vec_deriv -= np.dot(A.T, self.user_vectors)\n",
    "            bias_deriv -= np.expand_dims(np.sum(A, axis=0), 1)\n",
    "            # L2 regularization\n",
    "            vec_deriv -= self.reg_param * self.item_vectors\n",
    "        return (vec_deriv, bias_deriv)\n",
    "\n",
    "    # log 사후확률 (objective function)\n",
    "    def log_likelihood(self):\n",
    "        loglik = 0\n",
    "        A = np.dot(self.user_vectors, self.item_vectors.T)\n",
    "        A += self.user_biases\n",
    "        A += self.item_biases.T\n",
    "        B = A * self.counts\n",
    "        loglik += np.sum(B)\n",
    "\n",
    "        A = np.exp(A)\n",
    "        A += self.ones\n",
    "\n",
    "        A = np.log(A)\n",
    "        A = (self.counts + self.ones) * A\n",
    "        loglik -= np.sum(A)\n",
    "\n",
    "        # L2 regularization\n",
    "        loglik -= 0.5 * self.reg_param * np.sum(np.square(self.user_vectors))\n",
    "        loglik -= 0.5 * self.reg_param * np.sum(np.square(self.item_vectors))\n",
    "        return loglik\n",
    "\n",
    "    # print vecotrs\n",
    "    def print_vectors(self):\n",
    "        user_vecs_file = open('logmf-user-vecs-%i' % self.num_factors, 'w')\n",
    "        for i in range(self.num_users):\n",
    "            vec = ' '.join(map(str, self.user_vectors[i]))\n",
    "            line = '%i\\t%s\\n' % (i, vec)\n",
    "            user_vecs_file.write(line)\n",
    "        user_vecs_file.close()\n",
    "        item_vecs_file = open('logmf-item-vecs-%i' % self.num_factors, 'w')\n",
    "        for i in range(self.num_items):\n",
    "            vec = ' '.join(map(str, self.item_vectors[i]))\n",
    "            line = '%i\\t%s\\n' % (i, vec)\n",
    "            item_vecs_file.write(line)\n",
    "        item_vecs_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
